19 de Febrero 2020
- Hacer stemming con SnowballStemmer ("spanish")
- Implemetar frecuencia normalizada (Probabilidad).
	
	1. Stemming de Snowball + frec original
	2. Stemming de Snowball + probabilidad

----------------------------------------------------------
21 de Febrero 2020
Tenemos una lista de frecuencias:
grande = ('5, 0, 1, 3, 0, 5')

1. Pasar el vector de grande, con el de tf(frec) = [(k+1)*Frec]/(k + frec)
v_probab = vec_frecuencia/ np.sum(vec_frecuenncia)

v_tf = ((k + 1) * vec_frecuencias) / (vec_frecuencias + k)
k = 0.7, puede ser otro (Search value for k in BM25 = 1.2)

IDF(W) = log[(M+1)/K]

Para cada palabra del vocabulario ordenado, poner el número de contextos dónde esta esta palabra. (De nuestro diccionario de contextos):
Document_freq = (5, 1, 20, ..., 3)
k : Número de contextos donde esta la palabra W
M : Tamaño del vocabulario

tf - idf = tf * idf
vector_de_contexto = np.multiply(vec_tf * vec_idf)

	3. Stemming de Snowball + (tf - idf)


¿Qué tan compatibles son dos palabras en una oración?

--->>
Syntagmatic Relation = Correlated Ocurrences
X_w es la variable aleatoria

	4. Leer capítulo 6 y 13 del libro Text Data Management and Analysis.
	5. Descargar paquete de python GENSIM

---------------------------------------------------------
25 de Febrero 2020

Problemas de Frecuencia:
	1. Longitud distinta de documentos: probabilidad
	2. Palabras demasiado frecuentes en un doc: tf = BM25
	3. Palabras comunes: idf

** Adapting BM25 Retrival Model for Paradigmatic Relation Mining

----------------------------------------------------------
28 de Febrero 2020

1. Relación sintagmatica utilizando BM25.
2. Relación sintagmatica usando entropía condicional.


Menos entropía, menos incertidumbre.

Para entropía condicional.
--- Normalizar hasta quitar html tags.
	Tokenizar en oraciones.
	Normalizar cada oración.
	Poner tags.
	Lematizar.
	(Las oraciones salen de nltk.sent_tokenize)
	Ya tenemos lista de oraciones normalizadas.

------------------------------------------------------
04 Marzo 2020
Topic Mining and Analysis: Motivation
* Topic = main idea discussed in text data:
	- Theme/subject of a discussion or conversation
	- Different granularities
* Many applications require discovery of topics in text.
	- What are Twuitter users talking about today.
	- What are the current research topics in data mining? How are they differente from those 5 years ago?
	- What do people like about the iPhone 6? What do they dislike?
	- What were the major topics debated in 2012 presidential election?

* Task of Topic Mining and Analysis
	- Figure out which documents cover which topics.

HOMEWORK:
1. Generar la lista de frecuencia del texto. (mayor a menor)
	- No stopwords
	- Minuscula
	- Lematización
	- Filtrar por sustantivos, (sustantivos más frecuentes)
	-------
	vocabulario = set(tokens_normalizados)

2. Generar lista de frecuencia del texto. (mayor a menor)
	**** Cambiar IDF, como en la foto
	palabrasNormalizado: # de palabras en el archivo normalizado
	x: Frecuencia de la palabra
	TF = ((k+1)x) / (x+k)
	IDF = log(palabrasNormalizado / x)
	TF * IDF

3. 	Lista de títulos y subtitulos.
	// Ver si beutiful Soup

4. Computing Topic Coverage
	- Segmentar archivo html en articulos.
	- Sacar palabras lematizadas.
	- Lemmas de articulo.
	- Normalizar articulo.
	- Mostrar en forma de Tabla
	https://python-para-impacientes.blogspot.com/2017/01/tablas-con-estilo-con-tabulate.html

	Heuristics: Conocimiento común. No es un modelo matematico, es un simple conocimiento que tenemos.
	Por ejemplo, favorecer palabras en los títulos.
